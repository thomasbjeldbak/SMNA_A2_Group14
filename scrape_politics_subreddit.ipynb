{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d8eba2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fetching top posts from the last month...\n",
      "Processing last 30 days:   0%|                           | 0/30 [00:00<?, ?it/s]INFO:root:Filtering posts from 2024-10-18 to 2024-10-19\n",
      "INFO:root:Rate Limit Info: Used: 288, Remaining: 712.0, Reset in: 60 seconds\n",
      "Processing last 30 days:   3%|▌               | 1/30 [03:25<1:39:20, 205.53s/it]INFO:root:Filtering posts from 2024-10-17 to 2024-10-18\n",
      "INFO:root:Rate Limit Info: Used: 624, Remaining: 376.0, Reset in: 60 seconds\n",
      "Processing last 30 days:   7%|█               | 2/30 [06:47<1:34:50, 203.23s/it]INFO:root:Filtering posts from 2024-10-16 to 2024-10-17\n",
      "INFO:root:Rate Limit Info: Used: 785, Remaining: 215.0, Reset in: 60 seconds\n",
      "Processing last 30 days:  10%|█▌              | 3/30 [08:23<1:09:33, 154.57s/it]INFO:root:Filtering posts from 2024-10-15 to 2024-10-16\n",
      "INFO:root:Rate Limit Info: Used: 134, Remaining: 866.0, Reset in: 60 seconds\n",
      "Processing last 30 days:  13%|██▏             | 4/30 [11:53<1:16:22, 176.26s/it]INFO:root:Filtering posts from 2024-10-14 to 2024-10-15\n",
      "INFO:root:Rate Limit Info: Used: 464, Remaining: 536.0, Reset in: 60 seconds\n",
      "Processing last 30 days:  17%|██▋             | 5/30 [15:14<1:17:11, 185.26s/it]INFO:root:Filtering posts from 2024-10-13 to 2024-10-14\n",
      "INFO:root:Rate Limit Info: Used: 533, Remaining: 467.0, Reset in: 60 seconds\n",
      "Processing last 30 days:  20%|███▌              | 6/30 [15:52<54:03, 135.15s/it]INFO:root:Filtering posts from 2024-10-12 to 2024-10-13\n",
      "INFO:root:Rate Limit Info: Used: 812, Remaining: 188.0, Reset in: 60 seconds\n",
      "Processing last 30 days:  23%|████▏             | 7/30 [18:41<56:01, 146.17s/it]INFO:root:Filtering posts from 2024-10-11 to 2024-10-12\n",
      "INFO:root:Rate Limit Info: Used: 951, Remaining: 49.0, Reset in: 60 seconds\n",
      "Processing last 30 days:  27%|████▊             | 8/30 [20:03<46:07, 125.82s/it]INFO:root:Filtering posts from 2024-10-10 to 2024-10-11\n",
      "INFO:root:Rate Limit Info: Used: 143, Remaining: 857.0, Reset in: 60 seconds\n",
      "Processing last 30 days:  30%|█████▍            | 9/30 [21:59<43:00, 122.88s/it]INFO:root:Filtering posts from 2024-10-09 to 2024-10-10\n",
      "INFO:root:Rate Limit Info: Used: 628, Remaining: 372.0, Reset in: 60 seconds\n",
      "Processing last 30 days:  33%|█████▋           | 10/30 [26:49<58:08, 174.43s/it]INFO:root:Filtering posts from 2024-10-08 to 2024-10-09\n",
      "INFO:root:Rate Limit Info: Used: 822, Remaining: 178.0, Reset in: 60 seconds\n",
      "Processing last 30 days:  37%|██████▏          | 11/30 [28:46<49:35, 156.61s/it]INFO:root:Filtering posts from 2024-10-07 to 2024-10-08\n",
      "INFO:root:Rate Limit Info: Used: 909, Remaining: 91.0, Reset in: 60 seconds\n",
      "Processing last 30 days:  40%|██████▊          | 12/30 [29:38<37:27, 124.85s/it]INFO:root:Filtering posts from 2024-10-06 to 2024-10-07\n",
      "INFO:root:Rate Limit Info: Used: 203, Remaining: 797.0, Reset in: 60 seconds\n",
      "Processing last 30 days:  43%|███████▎         | 13/30 [32:34<39:47, 140.42s/it]INFO:root:Filtering posts from 2024-10-05 to 2024-10-06\n",
      "INFO:root:Rate Limit Info: Used: 320, Remaining: 680.0, Reset in: 60 seconds\n",
      "Processing last 30 days:  47%|███████▉         | 14/30 [33:45<31:52, 119.56s/it]INFO:root:Filtering posts from 2024-10-04 to 2024-10-05\n",
      "INFO:root:Rate Limit Info: Used: 578, Remaining: 422.0, Reset in: 60 seconds\n",
      "Processing last 30 days:  50%|████████▌        | 15/30 [36:20<32:31, 130.08s/it]INFO:root:Filtering posts from 2024-10-03 to 2024-10-04\n",
      "INFO:root:Rate Limit Info: Used: 917, Remaining: 83.0, Reset in: 60 seconds\n",
      "Processing last 30 days:  53%|█████████        | 16/30 [39:44<35:31, 152.26s/it]INFO:root:Filtering posts from 2024-10-02 to 2024-10-03\n",
      "WARNING:root:Too many requests when loading comments. Sleeping for 5 seconds...\n",
      "Processing last 30 days:  57%|█████████▋       | 17/30 [40:37<26:34, 122.66s/it]INFO:root:Filtering posts from 2024-10-01 to 2024-10-02\n",
      "INFO:root:Rate Limit Info: Used: 203, Remaining: 797.0, Reset in: 60 seconds\n",
      "Processing last 30 days:  60%|██████████▏      | 18/30 [42:34<24:10, 120.89s/it]INFO:root:Filtering posts from 2024-09-30 to 2024-10-01\n",
      "INFO:root:Rate Limit Info: Used: 327, Remaining: 673.0, Reset in: 60 seconds\n",
      "Processing last 30 days:  63%|██████████▊      | 19/30 [43:48<19:35, 106.85s/it]INFO:root:Filtering posts from 2024-09-29 to 2024-09-30\n",
      "INFO:root:Rate Limit Info: Used: 519, Remaining: 481.0, Reset in: 60 seconds\n",
      "Processing last 30 days:  67%|███████████▎     | 20/30 [45:43<18:13, 109.31s/it]INFO:root:Filtering posts from 2024-09-28 to 2024-09-29\n",
      "INFO:root:Rate Limit Info: Used: 774, Remaining: 226.0, Reset in: 60 seconds\n",
      "Processing last 30 days:  70%|███████████▉     | 21/30 [48:16<18:22, 122.47s/it]INFO:root:Filtering posts from 2024-09-27 to 2024-09-28\n",
      "WARNING:root:Too many requests when loading comments. Sleeping for 5 seconds...\n",
      "Processing last 30 days:  73%|████████████▍    | 22/30 [50:37<17:03, 127.99s/it]INFO:root:Filtering posts from 2024-09-26 to 2024-09-27\n",
      "INFO:root:Rate Limit Info: Used: 85, Remaining: 915.0, Reset in: 60 seconds\n",
      "Processing last 30 days:  77%|█████████████    | 23/30 [51:23<12:03, 103.42s/it]INFO:root:Filtering posts from 2024-09-25 to 2024-09-26\n",
      "INFO:root:Rate Limit Info: Used: 373, Remaining: 627.0, Reset in: 60 seconds\n",
      "Processing last 30 days:  80%|█████████████▌   | 24/30 [54:17<12:27, 124.57s/it]INFO:root:Filtering posts from 2024-09-24 to 2024-09-25\n",
      "INFO:root:Rate Limit Info: Used: 656, Remaining: 344.0, Reset in: 60 seconds\n",
      "Processing last 30 days:  83%|██████████████▏  | 25/30 [57:07<11:30, 138.07s/it]INFO:root:Filtering posts from 2024-09-23 to 2024-09-24\n",
      "INFO:root:Rate Limit Info: Used: 819, Remaining: 181.0, Reset in: 60 seconds\n",
      "Processing last 30 days:  87%|██████████████▋  | 26/30 [58:44<08:23, 125.77s/it]INFO:root:Filtering posts from 2024-09-22 to 2024-09-23\n",
      "WARNING:root:Too many requests when loading comments. Sleeping for 5 seconds...\n",
      "Processing last 30 days:  90%|█████████████▌ | 27/30 [1:00:37<06:06, 122.04s/it]INFO:root:Filtering posts from 2024-09-21 to 2024-09-22\n",
      "INFO:root:Rate Limit Info: Used: 250, Remaining: 750.0, Reset in: 60 seconds\n",
      "Processing last 30 days:  93%|██████████████ | 28/30 [1:03:03<04:17, 129.00s/it]INFO:root:Filtering posts from 2024-09-20 to 2024-09-21\n",
      "INFO:root:Rate Limit Info: Used: 892, Remaining: 108.0, Reset in: 60 seconds\n",
      "Processing last 30 days:  97%|██████████████▌| 29/30 [1:09:28<03:25, 205.81s/it]INFO:root:Filtering posts from 2024-09-19 to 2024-09-20\n",
      "WARNING:root:Too many requests when loading comments. Sleeping for 5 seconds...\n",
      "Processing last 30 days: 100%|███████████████| 30/30 [1:10:37<00:00, 141.25s/it]\n",
      "INFO:root:Total posts scraped: 26\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from redditClient import redditClient\n",
    "from prawcore.exceptions import TooManyRequests, RequestException\n",
    "import logging\n",
    "import json\n",
    "from tqdm import tqdm  # Import tqdm for progress monitoring\n",
    "\n",
    "# Configure logging to see detailed output\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def get_rate_limit_info(reddit):\n",
    "    \"\"\"\n",
    "    Fetch rate limit information from the last API response headers.\n",
    "    Handles missing 'reset' or 'remaining' keys.\n",
    "    \"\"\"\n",
    "    rate_limit_used = reddit.auth.limits.get('used', 0)\n",
    "    rate_limit_remaining = reddit.auth.limits.get('remaining', 100)  # Default to 100 if not present\n",
    "    rate_limit_reset = reddit.auth.limits.get('reset', 60)  # Default to 60 seconds if not present\n",
    "\n",
    "    return rate_limit_used, rate_limit_remaining, rate_limit_reset\n",
    "\n",
    "def fetch_replies(comment, max_replies=5, max_nested_replies=3):\n",
    "    \"\"\"\n",
    "    Recursively fetch replies to a comment, with limits on nested replies.\n",
    "    \n",
    "    @param comment: PRAW comment object\n",
    "    @param max_replies: Maximum number of direct replies to fetch\n",
    "    @param max_nested_replies: Maximum number of nested replies to fetch\n",
    "    @return: List of reply dictionaries\n",
    "    \"\"\"\n",
    "    replies_data = []\n",
    "    try:\n",
    "        comment.replies.replace_more(limit=None)  # Load all replies for the comment\n",
    "        replies = comment.replies.list()[:max_replies]  # Limit the number of direct replies\n",
    "\n",
    "        for reply in replies:\n",
    "            reply_data = {\n",
    "                'comment_id': reply.id,\n",
    "                'body': reply.body,\n",
    "                'author': reply.author.name if reply.author else '[deleted]',\n",
    "                'score': reply.score,\n",
    "                'created_utc': reply.created_utc,\n",
    "                'parent_id': reply.parent_id,\n",
    "                'replies': []\n",
    "            }\n",
    "            # Recursively fetch nested replies, with a limit on how deep to go\n",
    "            if max_nested_replies > 0:\n",
    "                reply_data['replies'] = fetch_replies(reply, max_replies=max_replies, max_nested_replies=max_nested_replies - 1)\n",
    "            replies_data.append(reply_data)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching replies: {e}\")\n",
    "    \n",
    "    return replies_data\n",
    "\n",
    "def scrape_top_daily_posts_comments(max_posts=20, max_comments=10, max_replies=5, max_nested_replies=3):\n",
    "    \"\"\"\n",
    "    Scrape the top posts for each day from r/politics over the last 30 days, including Redditor usernames and comments.\n",
    "    Includes nested replies with limits on the number of replies and nested replies.\n",
    "    \n",
    "    @param max_posts: Maximum number of posts to scrape per day\n",
    "    @param max_comments: Maximum number of comments to scrape per post\n",
    "    @param max_replies: Maximum number of replies per top-level comment\n",
    "    @param max_nested_replies: Maximum number of nested replies to fetch recursively\n",
    "    @return: List of dictionaries containing post and comment details\n",
    "    \"\"\"\n",
    "    # Initialize Reddit client\n",
    "    reddit = redditClient()\n",
    "\n",
    "    # Access the 'politics' subreddit\n",
    "    subreddit = reddit.subreddit('politics')\n",
    "\n",
    "    # Calculate the timestamp for 30 days ago\n",
    "    today = datetime.utcnow()\n",
    "    thirty_days_ago = today - timedelta(days=30)\n",
    "\n",
    "    # List to store the scraped data\n",
    "    scraped_data = []\n",
    "\n",
    "    # Fetch top posts for the last month (using 'month' time filter and manually filter posts per day)\n",
    "    try:\n",
    "        logging.info(\"Fetching top posts from the last month...\")\n",
    "        top_posts = list(subreddit.top(limit=None, time_filter='month'))\n",
    "\n",
    "        # Process posts and filter them by day\n",
    "        for day in tqdm(range(30), desc=\"Processing last 30 days\"):\n",
    "            day_end = today - timedelta(days=day)\n",
    "            day_start = day_end - timedelta(days=1)\n",
    "\n",
    "            logging.info(f\"Filtering posts from {day_start.strftime('%Y-%m-%d')} to {day_end.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "            # Filter posts for the specific day\n",
    "            daily_posts = [post for post in top_posts if day_start.timestamp() <= post.created_utc < day_end.timestamp()]\n",
    "\n",
    "            # If there are posts for the day, process the top post\n",
    "            if daily_posts:\n",
    "                top_post = max(daily_posts, key=lambda post: post.score)  # Get the top post by score\n",
    "                post_author = top_post.author.name if top_post.author else '[deleted]'\n",
    "\n",
    "                post_data = {\n",
    "                    'title': top_post.title,\n",
    "                    'id': top_post.id,\n",
    "                    'author': post_author,\n",
    "                    'score': top_post.score,\n",
    "                    'url': top_post.url,\n",
    "                    'num_comments': top_post.num_comments,\n",
    "                    'created_utc': top_post.created_utc,\n",
    "                    'upvote_ratio': top_post.upvote_ratio,\n",
    "                    'comments': []\n",
    "                }\n",
    "\n",
    "                # Fetch comments for each top post\n",
    "                time.sleep(1)  # 1 second delay to reduce rate limit issues\n",
    "                try:\n",
    "                    top_post.comments.replace_more(limit=None)\n",
    "                    top_comments = top_post.comments.list()[:max_comments]\n",
    "\n",
    "                    for comment in top_comments:\n",
    "                        comment_author = comment.author.name if comment.author else '[deleted]'\n",
    "                        comment_data = {\n",
    "                            'comment_id': comment.id,\n",
    "                            'body': comment.body,\n",
    "                            'author': comment_author,\n",
    "                            'score': comment.score,\n",
    "                            'created_utc': comment.created_utc,\n",
    "                            'parent_id': comment.parent_id,\n",
    "                            'replies': fetch_replies(comment, max_replies=max_replies, max_nested_replies=max_nested_replies)\n",
    "                        }\n",
    "                        post_data['comments'].append(comment_data)\n",
    "                except TooManyRequests:\n",
    "                    logging.warning(\"Too many requests when loading comments. Sleeping for 5 seconds...\")\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "\n",
    "                # Append post data to scraped_data\n",
    "                scraped_data.append(post_data)\n",
    "\n",
    "            # Monitor rate limit headers\n",
    "            used, remaining, reset_time = get_rate_limit_info(reddit)\n",
    "            logging.info(f\"Rate Limit Info: Used: {used}, Remaining: {remaining}, Reset in: {reset_time} seconds\")\n",
    "\n",
    "            if remaining < 5:\n",
    "                logging.info(f\"Approaching rate limit, sleeping for {reset_time} seconds...\")\n",
    "                time.sleep(reset_time)\n",
    "\n",
    "    except RequestException as e:\n",
    "        logging.error(f\"Request failed: {e}. Retrying...\")\n",
    "        retries = 0\n",
    "        while retries < 5:\n",
    "            retries += 1\n",
    "            logging.info(f\"Retrying... attempt {retries}/5\")\n",
    "            time.sleep(5 * retries)\n",
    "            try:\n",
    "                top_posts = subreddit.top(limit=max_posts, time_filter='month')\n",
    "                break\n",
    "            except RequestException:\n",
    "                if retries == 5:\n",
    "                    logging.error(\"Max retries exceeded. Aborting.\")\n",
    "                    raise\n",
    "\n",
    "    except TooManyRequests as e:\n",
    "        logging.warning(\"Hit the Reddit rate limit. Sleeping for 10 seconds...\")\n",
    "        time.sleep(10)\n",
    "\n",
    "    return scraped_data\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Scrape the data\n",
    "    data = scrape_top_daily_posts_comments(max_posts=20, max_comments=10, max_replies=5, max_nested_replies=3)\n",
    "\n",
    "    # Print the scraped data length to verify\n",
    "    logging.info(f\"Total posts scraped: {len(data)}\")\n",
    "\n",
    "    # Save the data to a JSON file for further analysis\n",
    "    with open(\"r_politics_top_daily_posts_and_comments.json\", \"w\") as outfile:\n",
    "        json.dump(data, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a179e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
