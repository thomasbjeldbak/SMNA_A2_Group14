{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4377345b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Rate Limit Info: Used: 209, Remaining: 791.0, Reset in: 60 seconds\n",
      "INFO:root:Rate Limit Info: Used: 377, Remaining: 623.0, Reset in: 60 seconds\n",
      "WARNING:root:Too many requests when loading comments. Sleeping for 5 seconds...\n",
      "INFO:root:Rate Limit Info: Used: 552, Remaining: 448.0, Reset in: 60 seconds\n",
      "INFO:root:Rate Limit Info: Used: 331, Remaining: 669.0, Reset in: 60 seconds\n",
      "WARNING:root:Too many requests when loading comments. Sleeping for 5 seconds...\n",
      "INFO:root:Rate Limit Info: Used: 551, Remaining: 449.0, Reset in: 60 seconds\n",
      "INFO:root:Rate Limit Info: Used: 169, Remaining: 831.0, Reset in: 60 seconds\n",
      "INFO:root:Rate Limit Info: Used: 360, Remaining: 640.0, Reset in: 60 seconds\n",
      "INFO:root:Rate Limit Info: Used: 488, Remaining: 512.0, Reset in: 60 seconds\n",
      "WARNING:prawcore:Retrying due to ConnectionError(ReadTimeoutError(\"HTTPSConnectionPool(host='oauth.reddit.com', port=443): Read timed out.\")) status: GET https://oauth.reddit.com/comments/1g1fjrj/_/lrgpnb0\n",
      "INFO:root:Rate Limit Info: Used: 303, Remaining: 697.0, Reset in: 60 seconds\n",
      "INFO:root:Rate Limit Info: Used: 46, Remaining: 954.0, Reset in: 60 seconds\n",
      "INFO:root:Total posts scraped: 977\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from redditClient import redditClient\n",
    "from prawcore.exceptions import TooManyRequests, RequestException\n",
    "import logging\n",
    "import networkx as nx\n",
    "import json\n",
    "\n",
    "# Configure logging to see detailed output\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def get_rate_limit_info(reddit):\n",
    "    \"\"\"\n",
    "    Fetch rate limit information from the last API response headers.\n",
    "    Handles missing 'reset' or 'remaining' keys.\n",
    "    \"\"\"\n",
    "    rate_limit_used = reddit.auth.limits.get('used', 0)\n",
    "    rate_limit_remaining = reddit.auth.limits.get('remaining', 100)  # Default to 100 if not present\n",
    "    rate_limit_reset = reddit.auth.limits.get('reset', 60)  # Default to 60 seconds if not present\n",
    "\n",
    "    return rate_limit_used, rate_limit_remaining, rate_limit_reset\n",
    "\n",
    "def scrape_r_politics_comments():\n",
    "    \"\"\"\n",
    "    Scrape posts and comments from r/politics from the last 30 days, including Redditor usernames.\n",
    "    Monitors rate limit headers to ensure compliance with Reddit API's rate limit.\n",
    "    \n",
    "    @return: List of dictionaries containing post and comment details\n",
    "    \"\"\"\n",
    "    # Initialize Reddit client\n",
    "    reddit = redditClient()\n",
    "\n",
    "    # Access the 'politics' subreddit\n",
    "    subreddit = reddit.subreddit('politics')\n",
    "\n",
    "    # Calculate the timestamp for 30 days ago\n",
    "    thirty_days_ago = datetime.utcnow() - timedelta(days=30)\n",
    "    thirty_days_ago_timestamp = time.mktime(thirty_days_ago.timetuple())\n",
    "\n",
    "    # List to store the scraped data\n",
    "    scraped_data = []\n",
    "\n",
    "    # Loop through posts using pagination\n",
    "    last_post = None\n",
    "\n",
    "    # Retry settings\n",
    "    max_retries = 5  # Maximum number of retries for failed requests\n",
    "    retry_delay = 5  # Initial delay before retrying (in seconds)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Fetch new posts with pagination, starting after the last post\n",
    "            posts = subreddit.new(limit=100, params={'after': last_post})\n",
    "\n",
    "            post_count = 0  # Count posts in this batch\n",
    "\n",
    "            for post in posts:\n",
    "                post_count += 1\n",
    "                # Check if the post was created in the last 30 days\n",
    "                if post.created_utc < thirty_days_ago_timestamp:\n",
    "                    return scraped_data  # Stop when reaching older posts\n",
    "\n",
    "                # Get the author's username or 'deleted' if the author is None\n",
    "                post_author = post.author.name if post.author else '[deleted]'\n",
    "\n",
    "                post_data = {\n",
    "                    'title': post.title,\n",
    "                    'id': post.id,\n",
    "                    'author': post_author,\n",
    "                    'score': post.score,\n",
    "                    'url': post.url,\n",
    "                    'num_comments': post.num_comments,\n",
    "                    'created_utc': post.created_utc,\n",
    "                    'upvote_ratio': post.upvote_ratio,\n",
    "                    'comments': []\n",
    "                }\n",
    "\n",
    "                # Fetch comments for each post\n",
    "                time.sleep(1)  # Add a 1 second delay to reduce chances of hitting rate limits\n",
    "                try:\n",
    "                    post.comments.replace_more(limit=None)  # To ensure all comments are loaded\n",
    "                except TooManyRequests:\n",
    "                    logging.warning(\"Too many requests when loading comments. Sleeping for 5 seconds...\")\n",
    "                    time.sleep(5)  # Sleep for a few seconds before retrying\n",
    "                    continue\n",
    "\n",
    "                for comment in post.comments.list():\n",
    "                    # Get the comment author's username or 'deleted' if the author is None\n",
    "                    comment_author = comment.author.name if comment.author else '[deleted]'\n",
    "\n",
    "                    comment_data = {\n",
    "                        'comment_id': comment.id,\n",
    "                        'body': comment.body,\n",
    "                        'author': comment_author,\n",
    "                        'score': comment.score,\n",
    "                        'created_utc': comment.created_utc,\n",
    "                        'parent_id': comment.parent_id  # Capture parent_id to preserve the comment thread structure\n",
    "                    }\n",
    "                    post_data['comments'].append(comment_data)\n",
    "\n",
    "                scraped_data.append(post_data)\n",
    "\n",
    "                # Update the last post's ID to continue pagination\n",
    "                last_post = post.fullname\n",
    "\n",
    "            # If no new posts were retrieved in this batch, stop the loop\n",
    "            if post_count == 0:\n",
    "                break\n",
    "\n",
    "            # Monitor rate limit headers after each batch of posts\n",
    "            used, remaining, reset_time = get_rate_limit_info(reddit)\n",
    "            logging.info(f\"Rate Limit Info: Used: {used}, Remaining: {remaining}, Reset in: {reset_time} seconds\")\n",
    "\n",
    "            # If the remaining rate limit is very low, pause until the reset time\n",
    "            if remaining < 5:\n",
    "                logging.info(f\"Approaching rate limit, sleeping for {reset_time} seconds...\")\n",
    "                time.sleep(reset_time)  # Sleep until rate limit reset\n",
    "\n",
    "        except RequestException as e:\n",
    "            logging.error(f\"Request failed: {e}. Retrying...\")\n",
    "            retries = 0\n",
    "            while retries < max_retries:\n",
    "                retries += 1\n",
    "                logging.info(f\"Retrying... attempt {retries}/{max_retries}\")\n",
    "                time.sleep(retry_delay * retries)  # Exponential backoff\n",
    "                try:\n",
    "                    posts = subreddit.new(limit=100, params={'after': last_post})\n",
    "                    break\n",
    "                except RequestException:\n",
    "                    if retries == max_retries:\n",
    "                        logging.error(\"Max retries exceeded. Aborting.\")\n",
    "                        raise\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "        except TooManyRequests as e:\n",
    "            # Handle Reddit API rate limiting by backing off and retrying\n",
    "            logging.warning(\"Hit the Reddit rate limit. Sleeping for 10 seconds...\")\n",
    "            time.sleep(10)  # Sleep for 10 seconds and then retry\n",
    "            continue\n",
    "\n",
    "        # Add a small delay between requests to prevent breaching rate limits\n",
    "        time.sleep(1)  # 1 second delay between each API call to stay safe\n",
    "\n",
    "    return scraped_data\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Scrape the data\n",
    "    data = scrape_r_politics_comments()\n",
    "\n",
    "    # Print the scraped data length to verify\n",
    "    logging.info(f\"Total posts scraped: {len(data)}\")\n",
    "\n",
    "    #save the data to a JSON file for further analysis\n",
    "    with open(\"r_politics_last_30_days_posts_and_comments.json\", \"w\") as outfile:\n",
    "        json.dump(data, outfile, indent=4)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
